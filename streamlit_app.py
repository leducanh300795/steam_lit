import streamlit as st
import pandas as pd
import numpy as np
import re
import string
import html
from gensim import corpora, models, similarities
from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.compose import ColumnTransformer
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import RandomOverSampler
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

# ============ PAGE SETUP ============
st.set_page_config(page_title="ITviec Data App", layout="wide")

# ============ SIDEBAR ============
with st.sidebar:
    st.title("Menu")
    page = st.radio(
        "Ch·ªçn trang",
        [
            "Gi·ªõi thi·ªáu",
            "Ph√¢n t√≠ch & K·∫øt qu·∫£",
            "Content-Based Similarity",
            "Recommend Classification",
            "Retrain Pipeline"
        ]
    )
    st.markdown("---")
    st.markdown("**Th√†nh vi√™n nh√≥m:**")
    st.markdown("- Mr. L√™ ƒê·ª©c Anh")
    st.markdown("Mail: leducanh300795@gmail.com")
    st.markdown("- Mr. Tr·∫ßn Anh T√∫")
    st.markdown("**GVHD:**")
    st.markdown("- Ms. Khu·∫•t Th√πy Ph∆∞∆°ng")
    st.markdown(
        "<div style='font-size: 11px; color: gray; text-align: center;'>"
        "D·ª± √°n t·ªët nghi·ªáp<br>Data Science & Machine Learning<br>TTTN - ƒêH KHTN"
        "</div>",
        unsafe_allow_html=True
    )

# ============ PAGE 1: GI·ªöI THI·ªÜU ============
if page == "Gi·ªõi thi·ªáu":
    st.image("banner_itviec_2.jpg", caption="Ngu·ªìn: ITviec")
    st.title("Gi·ªõi thi·ªáu")
    st.markdown("""
## V·ªÅ ITviec
ITViec l√† n·ªÅn t·∫£ng chuy√™n cung c·∫•p c∆° h·ªôi vi·ªác l√†m IT h√†ng ƒë·∫ßu Vi·ªát Nam. ·ª®ng vi√™n d·ªÖ d√†ng t√¨m ki·∫øm vi·ªác l√†m theo k·ªπ nƒÉng, ch·ª©c danh, c√¥ng ty; ƒë·ªìng th·ªùi tham kh·∫£o ƒë√°nh gi√°, blog chuy√™n ng√†nh, b√°o c√°o l∆∞∆°ng.

## B·ªô d·ªØ li·ªáu
H∆°n 8.000 ƒë√°nh gi√° t·ª´ nh√¢n vi√™n v√† c·ª±u nh√¢n vi√™n ng√†nh IT t·∫°i Vi·ªát Nam.

C√°c tr∆∞·ªùng ch√≠nh:
- `Company Name`
- `Company overview`
- `Our key skills`
- `Why you'll love working here`
- `like_cleaned` (review ƒë√£ clean)
- `rating_gap`
- `company_size`
- `Recommend?`

## M·ª•c ti√™u
- **Content-Based Similarity**: g·ª£i √Ω c√°c c√¥ng ty t∆∞∆°ng t·ª±
- **Recommend Classification**: d·ª± ƒëo√°n kh·∫£ nƒÉng recommend c√¥ng ty
    """)

# ============ PAGE 2: PH√ÇN T√çCH & K·∫æT QU·∫¢ ============
elif page == "Ph√¢n t√≠ch & K·∫øt qu·∫£":
    st.image("banner_itviec_2.jpg", caption="Ngu·ªìn: ITviec")
    st.title("üìä Ph√¢n t√≠ch & K·∫øt qu·∫£")
    st.markdown("""
## 1Ô∏è‚É£ Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu
- X√≥a gi√° tr·ªã thi·∫øu
- Chu·∫©n h√≥a lowercase, b·ªè HTML, b·ªè emoji, b·ªè d·∫•u c√¢u, k√Ω t·ª± ƒë·∫∑c bi·ªát
- Tokenization, lo·∫°i stopwords m·ªü r·ªông
- Unique token
- Vector h√≥a b·∫±ng TF-IDF (gensim)
- X√¢y d·ª±ng RandomForest v·ªõi pipeline
- X·ª≠ l√Ω m·∫•t c√¢n b·∫±ng nh√£n b·∫±ng RandomOverSampler

## 2Ô∏è‚É£ Content-Based Similarity
- Gh√©p text 3 tr∆∞·ªùng m√¥ t·∫£
- L√†m s·∫°ch nh∆∞ tr√™n
- TF-IDF
- MatrixSimilarity
- Input: t√™n c√¥ng ty ho·∫∑c m√¥ t·∫£ b·∫•t k·ª≥
- Top 5 c√¥ng ty t∆∞∆°ng t·ª±

## 3Ô∏è‚É£ Recommend Classification
- Gh√©p c√°c ƒë·∫∑c tr∆∞ng: gap features, categorical, text clean
- Pipeline:
  - CountVectorizer
  - StandardScaler
  - OneHotEncoder
- RandomForest + RandomOverSampler
- Cross-validation Accuracy ~88%
- Classification report hi·ªÉn th·ªã chi ti·∫øt

## 4Ô∏è‚É£ K·∫øt qu·∫£
- H·ªá th·ªëng ch·∫°y t·ªët, ph·∫£n h·ªìi nhanh
- Tri·ªÉn khai th·ª±c t·∫ø ti·ªÅm nƒÉng
    """)

# ============ PAGE 3: CONTENT-BASED SIMILARITY ============
elif page == "Content-Based Similarity":
    st.image("banner_itviec_2.jpg", caption="Ngu·ªìn: ITviec")
    st.title("üîç Content-Based Company Similarity")

    @st.cache_data
    def load_overview_data():
        df = pd.read_excel("Data/Overview_Companies.xlsx")
        df = df.dropna(subset=[
            "Company Name", "Company overview", "Our key skills", "Why you'll love working here"
        ])
        return df.reset_index(drop=True)

    df = load_overview_data()

    custom_stopwords = ENGLISH_STOP_WORDS.union({
        "https", "www", "com", "company", "job", "team"
    })

    def clean_text(text):
        if pd.isna(text):
            return ""
        text = html.unescape(text)
        text = re.sub(r'[^\x00-\x7F]+', ' ', text)
        text = text.lower()
        text = text.translate(str.maketrans('', '', string.punctuation))
        text = re.sub(r'\d+', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        words = [w for w in text.split() if w not in custom_stopwords]
        return ' '.join(dict.fromkeys(words))

    df["Company overview clean"] = df["Company overview"].apply(clean_text)
    df["Our key skills clean"] = df["Our key skills"].apply(clean_text)
    df["Why you'll love working here clean"] = df["Why you'll love working here"].apply(clean_text)

    df["full_text"] = df[
        ["Company overview clean", "Our key skills clean", "Why you'll love working here clean"]
    ].agg(" ".join, axis=1)

    df["tokens"] = df["full_text"].apply(lambda x: list(dict.fromkeys(x.split())))

    dictionary = corpora.Dictionary(df["tokens"])
    corpus = [dictionary.doc2bow(tokens) for tokens in df["tokens"]]
    tfidf_model = models.TfidfModel(corpus)
    index_tfidf = similarities.MatrixSimilarity(tfidf_model[corpus], num_features=len(dictionary))

    company_options = sorted(df["Company Name"].unique().tolist())
    company_options.insert(0, "Nh·∫≠p m√¥ t·∫£ b·∫•t k·ª≥")

    selected_company = st.selectbox("Ch·ªçn c√¥ng ty:", company_options)

    if selected_company == "Nh·∫≠p m√¥ t·∫£ b·∫•t k·ª≥":
        query_text = st.text_area("Nh·∫≠p m√¥ t·∫£ c√¥ng ty b·∫°n mu·ªën t√¨m t∆∞∆°ng t·ª±:")
    else:
        query_text = df[df["Company Name"] == selected_company]["full_text"].values[0]

    if st.button("T√¨m c√¥ng ty t∆∞∆°ng t·ª±"):
        cleaned_query = clean_text(query_text)
        query_tokens = list(dict.fromkeys(cleaned_query.split()))
        query_bow = dictionary.doc2bow(query_tokens)
        query_tfidf = tfidf_model[query_bow]
        sims = index_tfidf[query_tfidf]
        top_idx = sorted(enumerate(sims), key=lambda x: -x[1])[1:6]

        st.markdown("### üè¢ C√¥ng ty t∆∞∆°ng t·ª±:")
        for i, sim in top_idx:
            row = df.iloc[i]
            company = row["Company Name"]
            overview = row["Company overview"]
            skills = row["Our key skills"]
            why_love = row["Why you'll love working here"]

            st.subheader(f"{company}  (similarity={sims[i]:.2f})")
            st.markdown(f"**üìÑ Overview:** {overview}")
            st.markdown(f"**üõ†Ô∏è Skills:** {skills}")
            st.markdown(f"**üíñ Why you'll love working here:** {why_love}")
            st.markdown("---")

# ======================== PAGE 4: RECOMMEND CLASSIFICATION ========================
elif page == "Recommend Classification":
    st.image("banner_itviec_2.jpg", caption="Ngu·ªìn: ITviec")
    st.header("Recommend Classification")
    try:
        pipeline_rf = joblib.load("rf_pipeline_model_v2.joblib")
        pipeline_like = joblib.load("pipeline_like.pkl")
        df = pd.read_csv("data_for_classification.csv")
        overview = pd.read_excel("Data/Overview_Companies.xlsx")

        # T·∫°o danh s√°ch c√¥ng ty hi·ªÉn th·ªã ki·ªÉu: "ID - Company Name"
        company_options = overview[['id', 'Company Name']].drop_duplicates()
        company_options['display'] = company_options.apply(
            lambda x: f"{x['id']} - {x['Company Name']}", axis=1
        )
        selected_company_display = st.selectbox(
            "Ch·ªçn c√¥ng ty:",
            company_options['display'].tolist()
        )
        # t√°ch ra ID t·ª´ chu·ªói "id - name"
        company_id = int(selected_company_display.split(" - ")[0])

        st.subheader("ƒêi·ªÅn c√°c th√¥ng tin k·ª≥ v·ªçng")
        salary_benefits_exp = st.slider("Salary & Benefits", 1.0, 5.0, 3.0)
        training_learning_exp = st.slider("Training & Learning", 1.0, 5.0, 3.0)
        management_care_exp = st.slider("Management Care", 1.0, 5.0, 3.0)
        culture_fun_exp = st.slider("Culture & Fun", 1.0, 5.0, 3.0)

        rating_exp = np.mean([
            salary_benefits_exp,
            training_learning_exp,
            management_care_exp,
            culture_fun_exp
        ])
        rating_gap = 3.5 - rating_exp
        salary_benefits_gap = 3.5 - salary_benefits_exp
        training_learning_gap = 3.5 - training_learning_exp
        management_care_gap = 3.5 - management_care_exp
        culture_fun_gap = 3.5 - culture_fun_exp

        overtime_policy = st.selectbox("Overtime Policy", [
            "No OT", "Extra salary for OT", "Extra days off for OT"
        ])
        working_days = st.selectbox("Working Days", [
            "Monday - Friday", "Monday - Saturday"
        ])
        company_size = st.selectbox("Company Size", [
            "1-150 employees", "151-500 employees", ">500 employees"
        ])

        like_text = st.text_area("Mong ƒë·ª£i ∆∞u ƒëi·ªÉm c·ªßa c√¥ng ty:")
        if like_text.strip():
            like_topic = pipeline_like.predict([like_text])[0]
        else:
            like_topic = 0

        input_df = pd.DataFrame([{
            "id": company_id,
            "rating_gap": rating_gap,
            "salary_benefits_gap": salary_benefits_gap,
            "training_learning_gap": training_learning_gap,
            "management_care_gap": management_care_gap,
            "culture_fun_gap": culture_fun_gap,
            "overtime_policy": overtime_policy,
            "working_days": working_days,
            "company_size": company_size,
            "like_cleaned": like_text,
            "like_topic": like_topic
        }])

        if st.button("D·ª± ƒëo√°n"):
            y_pred = pipeline_rf.predict(input_df)
            y_proba = pipeline_rf.predict_proba(input_df)
            st.success(f"‚úÖ K·∫øt qu·∫£: {'Recommend' if y_pred[0]==1 else 'Not Recommend'}")
            st.info(f"X√°c su·∫•t Recommend: {y_proba[0][1]:.2%}")

    except Exception as e:
        st.error(f"L·ªói: {e}")


# ======================== PAGE 5: RETRAIN PIPELINE ========================
elif page == "Retrain Pipeline":
    st.header("Retrain Pipelines")

    if st.button("üîÅ Retrain pipeline_like"):
        data_TC = pd.read_csv("Reviews_cleaned_for_TC_v2.csv")
        pipeline_like = Pipeline([
            ("vectorizer", CountVectorizer(max_df=0.95, min_df=5)),
            ("lda", LatentDirichletAllocation(n_components=3, random_state=42)),
            ("kmeans", KMeans(n_clusters=3, random_state=42))
        ])
        pipeline_like.fit(data_TC['like_cleaned'])
        joblib.dump(pipeline_like, "pipeline_like.pkl")
        st.success("‚úÖ retrain pipeline_like th√†nh c√¥ng")

    if st.button("üîÅ Retrain pipeline_rf"):
        data = pd.read_csv("data_for_classification.csv")
        data['Recommend?'] = data['Recommend?'].map({'Yes': 1, 'No': 0})
        numerical_cols = [
            "rating_gap", "salary_benefits_gap", "training_learning_gap",
            "management_care_gap", "culture_fun_gap"
        ]
        categorical_cols = [
            "id", "overtime_policy", "working_days", "company_size", "like_topic"
        ]
        text_col = "like_cleaned"

        preprocessor = ColumnTransformer([
            ("num", StandardScaler(), numerical_cols),
            ("cat", OneHotEncoder(handle_unknown='ignore'), categorical_cols),
            ("text", CountVectorizer(max_features=5000), text_col)
        ])

        pipeline_rf = ImbPipeline([
            ("preprocessor", preprocessor),
            ("oversample", RandomOverSampler(random_state=42)),
            ("classifier", RandomForestClassifier(n_estimators=200, random_state=42))
        ])
        pipeline_rf.fit(data.drop("Recommend?", axis=1), data['Recommend?'])
        joblib.dump(pipeline_rf, "rf_pipeline_model_v2.joblib")
        st.success("‚úÖ retrain pipeline_rf th√†nh c√¥ng")